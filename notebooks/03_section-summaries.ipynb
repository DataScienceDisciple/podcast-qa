{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tiktoken\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = json.load(open('../config/openai_api.json', 'r'))[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Create a detailed and in-depth summary of this section of the Huberman Lab Podcast. Don't compress any information from the original transcript. If there is any information that should be added, please do so. Here is the transcript:\n",
    "\n",
    "\n",
    "{text}\n",
    "\n",
    "\n",
    "SUMMARY IN 10 BULLET POINTS:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_text_to_max_tokens(text: str, max_tokens: int = 4000, encoding_name: str = \"gpt-3.5-turbo\") -> str:\n",
    "    \"\"\"Truncate text from the file to a maximum number of tokens.\"\"\"\n",
    "    \n",
    "    def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "        \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "        encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "        num_tokens = len(encoding.encode(string))\n",
    "        return num_tokens\n",
    "    \n",
    "    current_num_tokens = num_tokens_from_string(text, encoding_name)\n",
    "\n",
    "    if current_num_tokens > max_tokens:\n",
    "        print(f'Text truncated, num tokens: {current_num_tokens}')\n",
    "        encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "        token_list = encoding.encode(text)\n",
    "        truncated_token_list = token_list[:max_tokens]\n",
    "        truncated_text = encoding.decode(truncated_token_list)\n",
    "    else:\n",
    "        print(f'Text not truncated, num tokens: {current_num_tokens}')\n",
    "        truncated_text = text\n",
    "\n",
    "    return truncated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_files_from_directory(input_directory, output_directory, prompt_template, model_name=\"gpt-3.5-turbo\"):\n",
    "    llm = ChatOpenAI(model_name=model_name)\n",
    "    BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template, \n",
    "                                        input_variables=[\"text\"])\n",
    "    chain = load_summarize_chain(llm,\n",
    "                             chain_type=\"stuff\",\n",
    "                             prompt=BULLET_POINT_PROMPT)\n",
    "    for filename in os.listdir(input_directory):\n",
    "        full_path = os.path.join(input_directory, filename)\n",
    "        save_path = os.path.join(output_directory, f'(Summary) {filename}')\n",
    "        if not os.path.exists(save_path):\n",
    "            print(f'Summarizing: {filename}')\n",
    "            with open(full_path) as f:\n",
    "                text = f.read()\n",
    "            text = truncate_text_to_max_tokens(text)\n",
    "            doc = [Document(page_content=text)]\n",
    "            output_summary = chain.run(doc)\n",
    "            with open(save_path, \"w\") as f:\n",
    "                f.write(output_summary)\n",
    "        else:\n",
    "            print(f\"{filename} already summarised!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = os.path.join('..', 'data', 'transcripts')\n",
    "output_dir = os.path.join('..', 'data', 'summaries')\n",
    "summarize_files_from_directory(input_dir, output_dir, prompt_template)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hubermanlab-qa-1tiWphOA-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
